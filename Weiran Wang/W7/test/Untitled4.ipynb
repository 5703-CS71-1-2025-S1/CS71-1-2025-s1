{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "0b176f0a-5819-4482-a172-c9774f3a6444",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Shape mismatch: if categories is an array, it has to be of shape (n_features,).",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[19], line 166\u001b[0m\n\u001b[0;32m    163\u001b[0m \u001b[38;5;66;03m# Main execution\u001b[39;00m\n\u001b[0;32m    164\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m    165\u001b[0m     \u001b[38;5;66;03m# Load and preprocess data\u001b[39;00m\n\u001b[1;32m--> 166\u001b[0m     X, y \u001b[38;5;241m=\u001b[39m load_dataset()\n\u001b[0;32m    168\u001b[0m     \u001b[38;5;66;03m# Split data into train, validation, and test sets\u001b[39;00m\n\u001b[0;32m    169\u001b[0m     X_train, X_test, y_train, y_test \u001b[38;5;241m=\u001b[39m train_test_split(X, y, test_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.2\u001b[39m, random_state\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m42\u001b[39m)\n",
      "Cell \u001b[1;32mIn[19], line 27\u001b[0m, in \u001b[0;36mload_dataset\u001b[1;34m()\u001b[0m\n\u001b[0;32m     22\u001b[0m \u001b[38;5;66;03m# Updated OneHotEncoder with sparse_output instead of sparse\u001b[39;00m\n\u001b[0;32m     23\u001b[0m nucleotide_encoder \u001b[38;5;241m=\u001b[39m OneHotEncoder(\n\u001b[0;32m     24\u001b[0m     categories\u001b[38;5;241m=\u001b[39m[[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mA\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mC\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mG\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mT\u001b[39m\u001b[38;5;124m'\u001b[39m]]\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m60\u001b[39m, \n\u001b[0;32m     25\u001b[0m     sparse_output\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m  \u001b[38;5;66;03m# Changed from sparse to sparse_output\u001b[39;00m\n\u001b[0;32m     26\u001b[0m )\n\u001b[1;32m---> 27\u001b[0m X_encoded \u001b[38;5;241m=\u001b[39m nucleotide_encoder\u001b[38;5;241m.\u001b[39mfit_transform(X_sequences)\n\u001b[0;32m     29\u001b[0m \u001b[38;5;66;03m# Encode class labels\u001b[39;00m\n\u001b[0;32m     30\u001b[0m label_encoder \u001b[38;5;241m=\u001b[39m LabelEncoder()\n",
      "File \u001b[1;32mC:\\ProgramData\\anaconda3\\Lib\\site-packages\\sklearn\\utils\\_set_output.py:313\u001b[0m, in \u001b[0;36m_wrap_method_output.<locals>.wrapped\u001b[1;34m(self, X, *args, **kwargs)\u001b[0m\n\u001b[0;32m    311\u001b[0m \u001b[38;5;129m@wraps\u001b[39m(f)\n\u001b[0;32m    312\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mwrapped\u001b[39m(\u001b[38;5;28mself\u001b[39m, X, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m--> 313\u001b[0m     data_to_wrap \u001b[38;5;241m=\u001b[39m f(\u001b[38;5;28mself\u001b[39m, X, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    314\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(data_to_wrap, \u001b[38;5;28mtuple\u001b[39m):\n\u001b[0;32m    315\u001b[0m         \u001b[38;5;66;03m# only wrap the first output for cross decomposition\u001b[39;00m\n\u001b[0;32m    316\u001b[0m         return_tuple \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m    317\u001b[0m             _wrap_data_with_container(method, data_to_wrap[\u001b[38;5;241m0\u001b[39m], X, \u001b[38;5;28mself\u001b[39m),\n\u001b[0;32m    318\u001b[0m             \u001b[38;5;241m*\u001b[39mdata_to_wrap[\u001b[38;5;241m1\u001b[39m:],\n\u001b[0;32m    319\u001b[0m         )\n",
      "File \u001b[1;32mC:\\ProgramData\\anaconda3\\Lib\\site-packages\\sklearn\\base.py:1098\u001b[0m, in \u001b[0;36mTransformerMixin.fit_transform\u001b[1;34m(self, X, y, **fit_params)\u001b[0m\n\u001b[0;32m   1083\u001b[0m         warnings\u001b[38;5;241m.\u001b[39mwarn(\n\u001b[0;32m   1084\u001b[0m             (\n\u001b[0;32m   1085\u001b[0m                 \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThis object (\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m) has a `transform`\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1093\u001b[0m             \u001b[38;5;167;01mUserWarning\u001b[39;00m,\n\u001b[0;32m   1094\u001b[0m         )\n\u001b[0;32m   1096\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m y \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m   1097\u001b[0m     \u001b[38;5;66;03m# fit method of arity 1 (unsupervised transformation)\u001b[39;00m\n\u001b[1;32m-> 1098\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfit(X, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mfit_params)\u001b[38;5;241m.\u001b[39mtransform(X)\n\u001b[0;32m   1099\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   1100\u001b[0m     \u001b[38;5;66;03m# fit method of arity 2 (supervised transformation)\u001b[39;00m\n\u001b[0;32m   1101\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfit(X, y, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mfit_params)\u001b[38;5;241m.\u001b[39mtransform(X)\n",
      "File \u001b[1;32mC:\\ProgramData\\anaconda3\\Lib\\site-packages\\sklearn\\base.py:1473\u001b[0m, in \u001b[0;36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[1;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1466\u001b[0m     estimator\u001b[38;5;241m.\u001b[39m_validate_params()\n\u001b[0;32m   1468\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[0;32m   1469\u001b[0m     skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n\u001b[0;32m   1470\u001b[0m         prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[0;32m   1471\u001b[0m     )\n\u001b[0;32m   1472\u001b[0m ):\n\u001b[1;32m-> 1473\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m fit_method(estimator, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mC:\\ProgramData\\anaconda3\\Lib\\site-packages\\sklearn\\preprocessing\\_encoders.py:976\u001b[0m, in \u001b[0;36mOneHotEncoder.fit\u001b[1;34m(self, X, y)\u001b[0m\n\u001b[0;32m    957\u001b[0m \u001b[38;5;129m@_fit_context\u001b[39m(prefer_skip_nested_validation\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m    958\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mfit\u001b[39m(\u001b[38;5;28mself\u001b[39m, X, y\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[0;32m    959\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    960\u001b[0m \u001b[38;5;124;03m    Fit OneHotEncoder to X.\u001b[39;00m\n\u001b[0;32m    961\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    974\u001b[0m \u001b[38;5;124;03m        Fitted encoder.\u001b[39;00m\n\u001b[0;32m    975\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 976\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_fit(\n\u001b[0;32m    977\u001b[0m         X,\n\u001b[0;32m    978\u001b[0m         handle_unknown\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandle_unknown,\n\u001b[0;32m    979\u001b[0m         force_all_finite\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mallow-nan\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    980\u001b[0m     )\n\u001b[0;32m    981\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_set_drop_idx()\n\u001b[0;32m    982\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_n_features_outs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compute_n_features_outs()\n",
      "File \u001b[1;32mC:\\ProgramData\\anaconda3\\Lib\\site-packages\\sklearn\\preprocessing\\_encoders.py:86\u001b[0m, in \u001b[0;36m_BaseEncoder._fit\u001b[1;34m(self, X, handle_unknown, force_all_finite, return_counts, return_and_ignore_missing_for_infrequent)\u001b[0m\n\u001b[0;32m     84\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcategories \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mauto\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m     85\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcategories) \u001b[38;5;241m!=\u001b[39m n_features:\n\u001b[1;32m---> 86\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m     87\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mShape mismatch: if categories is an array,\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     88\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m it has to be of shape (n_features,).\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     89\u001b[0m         )\n\u001b[0;32m     91\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcategories_ \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m     92\u001b[0m category_counts \u001b[38;5;241m=\u001b[39m []\n",
      "\u001b[1;31mValueError\u001b[0m: Shape mismatch: if categories is an array, it has to be of shape (n_features,)."
     ]
    }
   ],
   "source": [
    "# Import required libraries\n",
    "from ucimlrepo import fetch_ucirepo\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import LabelEncoder, OneHotEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Load dataset using ucimlrepo\n",
    "def load_dataset():\n",
    "    \"\"\"Fetch and preprocess the splice-junction gene sequences dataset\"\"\"\n",
    "    dataset = fetch_ucirepo(id=69)\n",
    "    \n",
    "    # Extract features and target\n",
    "    X = dataset.data.features\n",
    "    y = dataset.data.targets['class']\n",
    "    \n",
    "    # Convert DNA sequences to one-hot encoded matrix\n",
    "    X_sequences = X.iloc[:, 0].apply(lambda x: list(x)).values.tolist()  # Fixed index from Î¸ to 0\n",
    "    \n",
    "    # Updated OneHotEncoder with sparse_output instead of sparse\n",
    "    nucleotide_encoder = OneHotEncoder(\n",
    "        categories=[['A', 'C', 'G', 'T']]*60, \n",
    "        sparse_output=False  # Changed from sparse to sparse_output\n",
    "    )\n",
    "    X_encoded = nucleotide_encoder.fit_transform(X_sequences)\n",
    "    \n",
    "    # Encode class labels\n",
    "    label_encoder = LabelEncoder()\n",
    "    y_encoded = label_encoder.fit_transform(y)\n",
    "    \n",
    "    return X_encoded, y_encoded\n",
    "\n",
    "# PSO Optimizer with Early Stopping\n",
    "class PSO_MLP_Optimizer:\n",
    "    def __init__(self, X_train, X_val, y_train, y_val, \n",
    "                 n_particles=15, max_iter=50, c1=0.5, c2=0.3, w=0.9,\n",
    "                 early_stopping_rounds=5, tolerance=1e-4):\n",
    "        \"\"\"\n",
    "        Initialize PSO optimizer for MLP hyperparameter tuning\n",
    "        \n",
    "        Parameters:\n",
    "        - X_train, y_train: Training data\n",
    "        - X_val, y_val: Validation data for early stopping\n",
    "        - n_particles: Number of particles in the swarm\n",
    "        - max_iter: Maximum number of iterations\n",
    "        - c1, c2: Cognitive and social coefficients\n",
    "        - w: Inertia weight\n",
    "        - early_stopping_rounds: Stop if no improvement for N rounds\n",
    "        - tolerance: Minimum improvement to reset early stopping counter\n",
    "        \"\"\"\n",
    "        self.X_train = X_train\n",
    "        self.X_val = X_val\n",
    "        self.y_train = y_train\n",
    "        self.y_val = y_val\n",
    "        self.n_particles = n_particles\n",
    "        self.max_iter = max_iter\n",
    "        self.c1 = c1\n",
    "        self.c2 = c2\n",
    "        self.w = w\n",
    "        self.early_stopping_rounds = early_stopping_rounds\n",
    "        self.tolerance = tolerance\n",
    "        \n",
    "        # Search space boundaries\n",
    "        self.hidden_min = 10\n",
    "        self.hidden_max = 200\n",
    "        self.alpha_min = 1e-6\n",
    "        self.alpha_max = 1e-2\n",
    "        \n",
    "    def fitness_function(self, position):\n",
    "        \"\"\"Evaluate MLP performance for given hyperparameters\"\"\"\n",
    "        hidden_size = int(position[0])\n",
    "        alpha = 10**position[1]  # Convert from log scale\n",
    "        \n",
    "        # Create and train MLP with current parameters\n",
    "        mlp = MLPClassifier(\n",
    "            hidden_layer_sizes=(hidden_size,),\n",
    "            alpha=alpha,\n",
    "            max_iter=300,\n",
    "            early_stopping=True,\n",
    "            validation_fraction=0.1,\n",
    "            n_iter_no_change=10,\n",
    "            random_state=42\n",
    "        )\n",
    "        mlp.fit(self.X_train, self.y_train)\n",
    "        return accuracy_score(self.y_val, mlp.predict(self.X_val))\n",
    "    \n",
    "    def optimize(self):\n",
    "        \"\"\"Run PSO optimization with early stopping\"\"\"\n",
    "        # Initialize particles\n",
    "        particles_pos = np.array([\n",
    "            np.array([\n",
    "                np.random.uniform(self.hidden_min, self.hidden_max),\n",
    "                np.random.uniform(np.log10(self.alpha_min), np.log10(self.alpha_max))\n",
    "            ]) for _ in range(self.n_particles)\n",
    "        ])\n",
    "        \n",
    "        particles_vel = np.zeros((self.n_particles, 2))\n",
    "        particles_best_pos = particles_pos.copy()\n",
    "        particles_best_fitness = np.array([self.fitness_function(pos) for pos in particles_pos])\n",
    "        \n",
    "        global_best_idx = np.argmax(particles_best_fitness)\n",
    "        global_best_pos = particles_best_pos[global_best_idx]\n",
    "        global_best_fitness = particles_best_fitness[global_best_idx]\n",
    "        \n",
    "        # Early stopping variables\n",
    "        no_improvement_count = 0\n",
    "        best_iteration = 0\n",
    "        history = []\n",
    "        \n",
    "        # PSO main loop\n",
    "        for iteration in range(self.max_iter):\n",
    "            for i in range(self.n_particles):\n",
    "                # Update velocity\n",
    "                r1, r2 = np.random.rand(2)\n",
    "                cognitive = self.c1 * r1 * (particles_best_pos[i] - particles_pos[i])\n",
    "                social = self.c2 * r2 * (global_best_pos - particles_pos[i])\n",
    "                particles_vel[i] = self.w * particles_vel[i] + cognitive + social\n",
    "                \n",
    "                # Update position with bounds checking\n",
    "                particles_pos[i] += particles_vel[i]\n",
    "                particles_pos[i][0] = np.clip(particles_pos[i][0], self.hidden_min, self.hidden_max)\n",
    "                particles_pos[i][1] = np.clip(particles_pos[i][1], \n",
    "                                            np.log10(self.alpha_min), \n",
    "                                            np.log10(self.alpha_max))\n",
    "                \n",
    "                # Evaluate new position\n",
    "                current_fitness = self.fitness_function(particles_pos[i])\n",
    "                \n",
    "                # Update personal best\n",
    "                if current_fitness > particles_best_fitness[i]:\n",
    "                    particles_best_pos[i] = particles_pos[i].copy()\n",
    "                    particles_best_fitness[i] = current_fitness\n",
    "                    \n",
    "                    # Update global best\n",
    "                    if current_fitness > global_best_fitness + self.tolerance:\n",
    "                        global_best_pos = particles_pos[i].copy()\n",
    "                        global_best_fitness = current_fitness\n",
    "                        best_iteration = iteration\n",
    "                        no_improvement_count = 0\n",
    "                    elif current_fitness > global_best_fitness:\n",
    "                        # Small improvement that doesn't reset counter\n",
    "                        global_best_pos = particles_pos[i].copy()\n",
    "                        global_best_fitness = current_fitness\n",
    "            \n",
    "            # Early stopping check\n",
    "            no_improvement_count += 1\n",
    "            history.append(global_best_fitness)\n",
    "            \n",
    "            print(f\"Iteration {iteration+1}: Best Accuracy = {global_best_fitness:.4f}\")\n",
    "            \n",
    "            if no_improvement_count >= self.early_stopping_rounds:\n",
    "                print(f\"\\nEarly stopping triggered at iteration {iteration+1}\")\n",
    "                print(f\"No improvement for {self.early_stopping_rounds} consecutive iterations\")\n",
    "                break\n",
    "        \n",
    "        # Return best parameters and optimization history\n",
    "        best_hidden = int(global_best_pos[0])\n",
    "        best_alpha = 10**global_best_pos[1]\n",
    "        return best_hidden, best_alpha, global_best_fitness, history, best_iteration\n",
    "\n",
    "# Main execution\n",
    "if __name__ == \"__main__\":\n",
    "    # Load and preprocess data\n",
    "    X, y = load_dataset()\n",
    "    \n",
    "    # Split data into train, validation, and test sets\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "    X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.25, random_state=42)  # 0.25 x 0.8 = 0.2\n",
    "    \n",
    "    print(f\"Data shapes - Train: {X_train.shape}, Val: {X_val.shape}, Test: {X_test.shape}\")\n",
    "    \n",
    "    # Initialize and run PSO optimizer\n",
    "    pso_optimizer = PSO_MLP_Optimizer(\n",
    "        X_train=X_train,\n",
    "        X_val=X_val,\n",
    "        y_train=y_train,\n",
    "        y_val=y_val,\n",
    "        n_particles=15,\n",
    "        max_iter=50,\n",
    "        early_stopping_rounds=5,\n",
    "        tolerance=1e-4\n",
    "    )\n",
    "    \n",
    "    best_hidden, best_alpha, best_score, history, best_iter = pso_optimizer.optimize()\n",
    "    \n",
    "    print(f\"\\nOptimization Results:\")\n",
    "    print(f\"- Best hidden layer size: {best_hidden}\")\n",
    "    print(f\"- Best alpha: {best_alpha:.6f}\")\n",
    "    print(f\"- Validation accuracy: {best_score:.4f}\")\n",
    "    print(f\"- Best iteration: {best_iter + 1}\")\n",
    "    \n",
    "    # Train final model on combined train+val data with best parameters\n",
    "    final_mlp = MLPClassifier(\n",
    "        hidden_layer_sizes=(best_hidden,),\n",
    "        alpha=best_alpha,\n",
    "        max_iter=1000,\n",
    "        early_stopping=True,\n",
    "        validation_fraction=0.1,\n",
    "        n_iter_no_change=20,\n",
    "        random_state=42\n",
    "    )\n",
    "    \n",
    "    # Combine train and validation sets for final training\n",
    "    X_final = np.vstack([X_train, X_val])\n",
    "    y_final = np.concatenate([y_train, y_val])\n",
    "    \n",
    "    final_mlp.fit(X_final, y_final)\n",
    "    \n",
    "    # Evaluate on test set\n",
    "    test_acc = final_mlp.score(X_test, y_test)\n",
    "    print(f\"\\nFinal Model Performance:\")\n",
    "    print(f\"- Training accuracy: {final_mlp.score(X_final, y_final):.4f}\")\n",
    "    print(f\"- Test accuracy: {test_acc:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78d0c727-73f3-40d9-b490-931e2bdf3257",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
